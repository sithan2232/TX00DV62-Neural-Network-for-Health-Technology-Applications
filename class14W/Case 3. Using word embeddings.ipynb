{"cells":[{"metadata":{"_uuid":"d4ff3d6a6cf9f7af7f598061b5d80151ea40f645"},"cell_type":"markdown","source":"# Case 3. Using word embeddings\nNeural networks for Health Technology Applications<br>\n26.2.2020, Sakari Lukkarinen<br>\n[Helsinki Metropolia University of Applied Sciences](www.metropolia.fi/en)"},{"metadata":{"_uuid":"980d820d4d9b760a09df5f240ef36be473e457fb"},"cell_type":"markdown","source":"## Introduction\n\nThe aim of this Notebook is to work as introduction to text preprocessing functions for neural networks.\n\n## Acknowledgments\n\nThe dataset is from: [UCI ML Drug Review dataset](https://www.kaggle.com/jessicali9530/kuc-hackathon-winter-2018)."},{"metadata":{},"cell_type":"markdown","source":"![](http://)## Import libraries and read the datasets"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Read the basic libraries (similar start as in Kaggle kernels)\n%pylab inline\nimport time # for timing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split # preprocessing datasets\nfrom tensorflow.keras.preprocessing.text import Tokenizer # text preprocessing\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # text preprocessing\nfrom tensorflow.keras.models import Sequential # modeling neural networks\nfrom tensorflow.keras.layers import Dense, Activation # layers for neural networks\nfrom sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score # final metrics\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff3416f3c083d6d6e9b38c6d2f27671b99ea8a5b","trusted":true},"cell_type":"code","source":"# Change the default figure size\nplt.rcParams['figure.figsize'] = [12, 5]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Create dataframes train and test\ntrain = pd.read_csv('../input/drugsComTrain_raw.csv')\ntest = pd.read_csv('../input/drugsComTest_raw.csv')\n\n# Show the first 5 rows of the train set\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text processing\n\nMore info: \n- [scikit-learn CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n- [scikit-learn text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n- [keras Tokenizer]()\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Tokenize the text\nsamples = train['review']\ntokenizer = Tokenizer(num_words = 5000)\ntokenizer.fit_on_texts(samples)\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(samples)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Create three categories\n# label = 4, when rating == 10\n# label = 3, when rating == 8...9\n# label = 2, when rating = 5..7\n# label = 1, when rating = 2..4\n# label = 0, when rating = 1\nlabels = train['rating'].values\nfor i in range(len(labels)):\n    x = labels[i]\n    if x == 10:\n        labels[i] = 4\n    elif x >= 8:\n        labels[i] = 3\n    elif x >= 5:\n        labels[i] = 2\n    elif x >= 2:\n        labels[i] = 1\n    else:\n        labels[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VALIDATION_SPLIT = 0.25\n\n# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import Constant\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(5000,\n                            100,\n                            input_length=200,\n                            trainable=True)\n\nsequence_input = Input(shape=(200,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu')(x)\npreds = Dense(5, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory = model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=10,\n          validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the accuracy and loss\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ne = arange(len(acc)) + 1\n\nplot(e, acc, label = 'train')\nplot(e, val_acc, label = 'validation')\ntitle('Training and validation accuracy')\nxlabel('Epoch')\ngrid()\nlegend()\n\nfigure()\n\nplot(e, loss, label = 'train')\nplot(e, val_loss, label = 'validation')\ntitle('Training and validation loss')\nxlabel('Epoch')\ngrid()\nlegend()\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the predicted values for the validation set\ny_pred = argmax(model.predict(x_val), axis = 1)\ny_true = argmax(y_val, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the classification report\ncr = classification_report(y_true, y_pred)\nprint(cr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the confusion matrix\ncm = confusion_matrix(y_true, y_pred).T\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the cohen's kappa, both with linear and quadratic weights\nk = cohen_kappa_score(y_true, y_pred)\nprint(f\"Cohen's kappa (linear)    = {k:.3f}\")\nk2 = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\nprint(f\"Cohen's kappa (quadratic) = {k2:.3f}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More info: \n- [sklearn.metrics.cohen_kappa_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)\n- [Cohen's kappa (Wikipedia)](https://en.wikipedia.org/wiki/Cohen%27s_kappa)"},{"metadata":{},"cell_type":"markdown","source":"## Next step\nTo use Glove word embeddings to fix the embedded layer. For more details see:\n    https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = \"\"\"\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\nfrom keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":4}